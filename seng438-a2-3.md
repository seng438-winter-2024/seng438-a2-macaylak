**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#2 – Requirements-Based Test Generation**

| Group: 3                  |
|---------------------------|
| Student 1 Ahmed Abbas     |   
| Student 2 Rimal Rizvi     |   
| Student 3 Mariyah Malik   |   
| Student 4 Macayla Konig   |   

# 1 Introduction
In this lab assignment, our main goal is to explore the fundamentals of automated unit testing, with a specific focus on crafting tests tailored to the distinct requirements of each unit. JUnit, an integral part of the XUnit framework family and a widely used unit testing framework for Java, takes center stage as our primary tool. By the end of this lab, we aim to empower ourselves to create automated test code proficiently using JUnit and other XUnit frameworks, such as NUnit. Additionally, we will learn to seamlessly integrate and work with mock objects in the development of our test code.

It's important to note that this lab is a collaborative effort, meant to be completed as a group. Together, we will navigate through three key sections. The initial phase is designed to familiarize ourselves with the essential concepts. We'll then transition to the creation of unit tests based on the requirements outlined in Javadocs. Finally, the testing journey concludes with the execution of the test suites on various versions of the system under scrutiny, with a meticulous collection of the results.

The tools at our disposal for this lab are JUnit and Javadoc. JUnit, being a popular and free unit testing tool for Java, provides a robust framework for our testing endeavors. Javadoc, on the other hand, serves as the format for storing requirement specifications, aiding in the generation of comprehensive test suites.

# 2 Detailed description of unit test strategy

The unit test strategy for this lab is to develop a comprehensive suite of tests that cover all the requirements outlined in the Javadoc. The strategy is to create a test suite for each method in the source code, with each test suite containing test cases that cover all the partitions identified in the requirements. The test cases will be developed using JUnit, and the test suites will be executed on various versions of the system under scrutiny. The test strategy will be implemented in the following steps:

1. **Familiarize with the requirements**: The first step is to familiarize ourselves with the requirements outlined in the Javadoc. We will carefully read through the requirements and identify the partitions for each method in the source code.

2. **Develop test cases**: The next step is to develop test cases for each partition identified in the requirements. We will use JUnit to create test cases that cover all the partitions for each method in the source code.

3. **Create test suites**: Once the test cases are developed, we will create test suites for each method in the source code. Each test suite will contain test cases that cover all the partitions identified in the requirements.

4. **Execute test suites**: The final step is to execute the test suites on various versions of the system under scrutiny. We will collect the results of the test suites and analyze the test coverage to ensure that all the requirements are covered.

# 3 Test cases developed

Text…

// write down the name of the test methods and classes. Organize the based on
the source code method // they test. identify which tests cover which partitions
you have explained in the test strategy section //above

# 4 How the team work/effort was divided and managed

Our team: Ahmed, Macayla, Mariyah, and Rimal, implemented a hybrid approach that merged individual tasks with pair testing. This strategy, facilitated through remote collaboration via Discord, harnessed our collective strengths, enabling not only the swift execution of the lab tasks but also an enhanced understanding of the DataUtilities and Range classes through joint learning and evaluation.

Responsibilities for developing unit tests for specific methods within these classes were allocated as follows, ensuring comprehensive functionality testing:

Ahmed concentrated on the following:
- DataUtilities: calculateColumnTotal, calculateRowTotal, createNumberArray
- Range: expand, expandToInclude

Macayla covered:
- DataUtilities: getCumulativePercentages, calculateColumnTotal, equal
- Range: contains, combine, getLength, shift

Mariyah was assigned:
- DataUtilities: createNumberArray, createNumberArray2D
- Range: toString, expandToInclude

Rimal managed:
- DataUtilities: createNumberArray
- Range: equals, constrain, intersects

Our method of dividing the work: aiming to minimize overlap and ensure detailed test coverage, was further enhanced by our focus on boundary conditions and output validation, utilizing the equivalence class technique to assess method performance under various scenarios.

Following the development of our tests, we conducted detailed peer reviews, both within our pairs and as a complete team, sharing our screens and discussing our observations in real-time on Discord. This collaborative effort not only accelerated our lab work but also deepened our comprehension and application of unit testing principles, particularly the importance of boundary conditions and the effectiveness of pair testing in collaborative troubleshooting.

Ultimately, our joint efforts, structured around both pair testing and individual contributions, led to the development of an extensive suite of unit tests for the DataUtilities and Range classes. This method facilitated not only the efficient completion of our lab but also fostered a more profound collective insight into the software under test (SUT).


# 5 Difficulties encountered, challenges overcome, and lessons learned

We faced a series of challenges and learning opportunities during the lab assignment, facing technical issues and the complexities of working with mock objects and ambiguous documentation. These experiences shed light on the intricate nature of unit testing in practical scenarios, highlighting the need for adaptability and comprehensive understanding in software testing.

Ahmed experienced difficulties in determining test partitions and boundary values due to the limited or unclear documentation, especially with the `calculateColumnTotal` method in the DataUtilities class. This vagueness in documentation necessitated reliance on the tester's judgment, showcasing the challenges posed by inconsistent or lacking specifications.

Macayla encountered obstacles with the `getCumulativePercentages` function, particularly with mocking the KeyedValues object parameter, due to the absence of a straightforward method to create and assign values without seeing the actual code. This situation brought to light the challenges of black box testing and the potential inefficiencies and confusion arising from extensive mocking requirements.

Mariyah observed an increased level of difficulty compared to previous assignments, noting varied approaches to testing methods like `createNumberArray`. This situation emphasized the learning process and the need for collaborative efforts to tackle complex testing scenarios.

Rimal pointed out the challenges in devising boundary values and ensuring comprehensive case coverage, particularly with functions like `intersects` and `equals` in the Range class. The encounter with unforeseen bugs highlighted the effectiveness of parameterized tests in efficiently handling redundant test scenarios.

Our team also faced technical hurdles, including missing JAR files crucial for mocking objects in the DataUtilitiesTest class and issues with running JFreeChart demos on MacBook laptops, prompting a switch to Windows platforms. The complexity of the lab documentation and the initial challenges in effectively distributing work among team members were resolved by adopting a pair testing strategy, fostering better collaboration and efficiency.

The employment of mock objects through jMock was instrumental in our testing strategy, enabling the simulation of complex real object behaviors in a controlled setting. Despite its benefits, mocking introduced challenges, such as discrepancies between mock and real object behaviors and the complexities of mocking without code access. These experiences have taught our team the significance of flexibility, the risks associated with relying on incomplete documentation, and both the advantages and limitations of mocking in unit testing.


# 6 Comments/feedback on the lab itself

We faced challenges during our lab assignment stemming from discrepancies in materials provided on D2L and GitHub. These inconsistencies, especially in lab document versions and provided artifacts, led to confusion and inconsistent test outcomes among team members. Nonetheless, the lab served as an instructive introduction to black-box testing techniques using JUnit and JMock, underscoring the critical role of precise documentation and effective communication in software projects.

Each team member derived constructive insights from the lab: Ahmed highlighted the confusion caused by inconsistent materials; Macayla valued the introduction to JUnit and JMock; Mariyah appreciated the instructions as a primer for automated testing; and Rimal relished the practical experience, particularly the insights gained in unit testing and the use of mocking. Our experiences emphasize the necessity for uniform documentation across educational platforms and the importance of proactive communication to mitigate confusion. The prompt assistance from teaching assistants and the professor was instrumental in navigating these challenges, suggesting that regular updates to materials and clear instructions could enhance both the efficiency and the educational impact of future labs.

